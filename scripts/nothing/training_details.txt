adwqd
**** TRAINING RUN, datetime: 2025-03-28 17:10:32 ****============================================
Training setup:
{
    "model_path": "./test3-tiny-prompt/checkpoint-980",
    "continue_from_checkpoint": false,
    "train_datasets": [
        "atco_en"
    ],
    "path_to_train_datasets": "./data",
    "use_prompt": true,
    "self_prompt": true,
    "transcription_name_in_ds": "full_ts",
    "prompt_name_in_ds": "prompt_fullts_1G_4B"
}
Training arguments:
{
    "output_dir": "./nothing",
    "per_device_train_batch_size": 4,
    "gradient_accumulation_steps": 4,
    "learning_rate": 1e-05,
    "warmup_ratio": 0.12,
    "gradient_checkpointing": true,
    "fp16": true,
    "save_strategy": "epoch",
    "num_train_epochs": 10,
    "per_device_eval_batch_size": 8,
    "predict_with_generate": true,
    "generation_max_length": 448,
    "logging_steps": 30,
    "report_to": [
        "none"
    ],
    "metric_for_best_model": "wer",
    "greater_is_better": false,
    "push_to_hub": false
}

**** TRAINING RUN, datetime: 2025-03-28 17:15:49
 ****============================================
Training setup:
{
    "model_path": "./test3-tiny-prompt/checkpoint-980",
    "continue_from_checkpoint": true,
    "train_datasets": [
        "atco_en"
    ],
    "path_to_train_datasets": "./data",
    "use_prompt": true,
    "self_prompt": false,
    "transcription_name_in_ds": "full_ts",
    "prompt_name_in_ds": "prompt_fullts_1G_4B"
}
Training arguments:
{
    "output_dir": "./nothing",
    "per_device_train_batch_size": 4,
    "gradient_accumulation_steps": 4,
    "learning_rate": 1e-05,
    "warmup_ratio": 0.12,
    "gradient_checkpointing": true,
    "fp16": true,
    "save_strategy": "epoch",
    "num_train_epochs": 10,
    "per_device_eval_batch_size": 8,
    "predict_with_generate": true,
    "generation_max_length": 448,
    "logging_steps": 30,
    "report_to": [
        "none"
    ],
    "metric_for_best_model": "wer",
    "greater_is_better": false,
    "push_to_hub": false
}

Training finished, datetime: 2025-03-28 17:15:49
 ****'
============================================
**** TRAINING RUN, datetime: 2025-03-28 17:16:21
 ****============================================
Training setup:
{
    "model_path": "./test3-tiny-prompt/checkpoint-980",
    "continue_from_checkpoint": false,
    "train_datasets": [
        "atco_en"
    ],
    "path_to_train_datasets": "./data",
    "use_prompt": true,
    "self_prompt": false,
    "transcription_name_in_ds": "full_ts",
    "prompt_name_in_ds": "prompt_fullts_1G_4B"
}
Training arguments:
{
    "output_dir": "./nothing",
    "per_device_train_batch_size": 4,
    "gradient_accumulation_steps": 4,
    "learning_rate": 1e-05,
    "warmup_ratio": 0.12,
    "gradient_checkpointing": true,
    "fp16": true,
    "save_strategy": "epoch",
    "num_train_epochs": 10,
    "per_device_eval_batch_size": 8,
    "predict_with_generate": true,
    "generation_max_length": 448,
    "logging_steps": 30,
    "report_to": [
        "none"
    ],
    "metric_for_best_model": "wer",
    "greater_is_better": false,
    "push_to_hub": false
}

**** TRAINING RUN, datetime: 2025-04-02 23:09:32
 ****============================================
Training setup:
{
    "model_path": "openai/whisper-tiny",
    "continue_from_checkpoint": false,
    "train_datasets": [
        "atco_test_en_ruzyne"
    ],
    "eval_datasets": [
        "atco_en_ruzyne"
    ],
    "datasets_root_dir": "./data",
    "use_prompt": false,
    "self_prompt": false,
    "transcription_name_in_ds": "full_ts",
    "prompt_name_in_ds": "prompt_fullts_1G_4B"
}
Training arguments:
{
    "output_dir": "./nothing",
    "per_device_train_batch_size": 4,
    "gradient_accumulation_steps": 4,
    "learning_rate": 1e-05,
    "warmup_ratio": 0.12,
    "gradient_checkpointing": true,
    "fp16": true,
    "save_strategy": "epoch",
    "num_train_epochs": 2,
    "per_device_eval_batch_size": 4,
    "predict_with_generate": true,
    "generation_max_length": 448,
    "logging_steps": 30,
    "report_to": [
        "none"
    ],
    "metric_for_best_model": "wer",
    "greater_is_better": false,
    "push_to_hub": false
}

**** TRAINING RUN, datetime: 2025-04-02 23:17:08
 ****============================================
Training setup:
{
    "model_path": "openai/whisper-tiny",
    "continue_from_checkpoint": false,
    "train_datasets": [
        "atco_test_en_ruzyne"
    ],
    "eval_datasets": [
        "atco_en_ruzyne"
    ],
    "datasets_root_dir": "./data",
    "use_prompt": false,
    "self_prompt": false,
    "transcription_name_in_ds": "full_ts",
    "prompt_name_in_ds": "prompt_fullts_1G_4B"
}
Training arguments:
{
    "output_dir": "./nothing",
    "per_device_train_batch_size": 4,
    "gradient_accumulation_steps": 4,
    "learning_rate": 1e-05,
    "warmup_ratio": 0.12,
    "gradient_checkpointing": true,
    "fp16": true,
    "eval_strategy": "epoch",
    "save_strategy": "epoch",
    "num_train_epochs": 2,
    "per_device_eval_batch_size": 4,
    "predict_with_generate": true,
    "generation_max_length": 448,
    "logging_steps": 30,
    "report_to": [
        "tensorboard"
    ],
    "metric_for_best_model": "wer",
    "greater_is_better": false,
    "push_to_hub": false
}

Training finished, datetime: 2025-04-02 23:17:55
 ****'
============================================
**** TRAINING RUN, datetime: 2025-04-02 23:27:46
 ****============================================
Training setup:
{
    "model_path": "openai/whisper-tiny",
    "continue_from_checkpoint": false,
    "train_datasets": [
        "atco_test_en_ruzyne"
    ],
    "eval_datasets": [
        "atco_en_ruzyne"
    ],
    "datasets_root_dir": "./data",
    "use_prompt": false,
    "self_prompt": false,
    "transcription_name_in_ds": "full_ts",
    "prompt_name_in_ds": "prompt_fullts_1G_4B"
}
Training arguments:
{
    "output_dir": "./nothing",
    "per_device_train_batch_size": 4,
    "gradient_accumulation_steps": 4,
    "learning_rate": 1e-05,
    "warmup_ratio": 0.12,
    "gradient_checkpointing": true,
    "fp16": true,
    "eval_strategy": "epoch",
    "save_strategy": "epoch",
    "num_train_epochs": 2,
    "per_device_eval_batch_size": 4,
    "predict_with_generate": true,
    "generation_max_length": 448,
    "logging_steps": 30,
    "report_to": [
        "tensorboard"
    ],
    "metric_for_best_model": "wer",
    "greater_is_better": false,
    "push_to_hub": false
}

Training finished, datetime: 2025-04-02 23:28:32
 ****'
============================================
**** TRAINING RUN, datetime: 2025-04-02 23:32:39
 ****============================================
Training setup:
{
    "model_path": "openai/whisper-small",
    "continue_from_checkpoint": false,
    "train_datasets": [
        "atco_test_en_ruzyne"
    ],
    "eval_datasets": [
        "atco_en_ruzyne"
    ],
    "datasets_root_dir": "./data",
    "use_prompt": false,
    "self_prompt": false,
    "transcription_name_in_ds": "full_ts",
    "prompt_name_in_ds": "prompt_fullts_1G_4B"
}
Training arguments:
{
    "output_dir": "./nothing",
    "per_device_train_batch_size": 4,
    "gradient_accumulation_steps": 4,
    "learning_rate": 1e-05,
    "warmup_ratio": 0.12,
    "gradient_checkpointing": true,
    "fp16": true,
    "eval_strategy": "epoch",
    "save_strategy": "epoch",
    "num_train_epochs": 2,
    "per_device_eval_batch_size": 4,
    "predict_with_generate": true,
    "generation_max_length": 448,
    "logging_steps": 30,
    "report_to": [
        "tensorboard"
    ],
    "metric_for_best_model": "wer",
    "greater_is_better": false,
    "push_to_hub": false
}

Training finished, datetime: 2025-04-02 23:34:10
 ****'
============================================
**** TRAINING RUN, datetime: 2025-04-02 23:34:43
 ****============================================
Training setup:
{
    "model_path": "openai/whisper-small",
    "continue_from_checkpoint": false,
    "train_datasets": [
        "atco_test_en_ruzyne"
    ],
    "eval_datasets": [
        "atco_en_ruzyne"
    ],
    "datasets_root_dir": "./data",
    "use_prompt": false,
    "self_prompt": false,
    "transcription_name_in_ds": "full_ts",
    "prompt_name_in_ds": "prompt_fullts_1G_4B"
}
Training arguments:
{
    "output_dir": "./nothing",
    "per_device_train_batch_size": 4,
    "gradient_accumulation_steps": 4,
    "learning_rate": 1e-05,
    "warmup_ratio": 0.12,
    "gradient_checkpointing": true,
    "fp16": true,
    "eval_strategy": "epoch",
    "save_strategy": "epoch",
    "num_train_epochs": 2,
    "per_device_eval_batch_size": 4,
    "predict_with_generate": true,
    "generation_max_length": 448,
    "logging_steps": 30,
    "report_to": [
        "tensorboard"
    ],
    "metric_for_best_model": "wer",
    "greater_is_better": false,
    "push_to_hub": false
}

Training finished, datetime: 2025-04-02 23:36:13
 ****'
============================================
